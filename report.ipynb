{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***FCIM.FIA - Fundamentals of Artificial Intelligence***\n",
    "> Lab 4: Natural Language Processing and Chat Bots \\\n",
    "> Performed by: Dobrojan Alexandru, FAF-212 \\\n",
    "> Verified by: Elena Graur, asist. univ.\n"
   ],
   "id": "bf087999b3dc87e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Theory",
   "id": "d7a9c3f64e18ca81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In machine learning, NLP helps computers interpret text and speech in a way that enables them to perform tasks like translation, sentiment analysis, text summarization, and chatbot communication. NLP involves converting unstructured language data like sentences into a structured format that computers can work with, such as numbers or vectors. This is done using techniques like tokenization, stemming, and feature extraction.\n",
    "\n",
    "LSTMs could be an ideal solution for this task because it excels in processing sequential data and understanding context over time. Unlike a simple feedforward neural network, LSTMs can capture dependencies between words in a sentence by maintaining a memory of previous inputs, which is crucial in natural language processing tasks. For instance, LSTMs can handle longer sentences where the meaning of a word depends on the context provided by preceding words. Additionally, they can better model variations in sentence structure, making them more robust to user input that doesn't strictly match predefined patterns.\n",
    "\n",
    "This laboratory work implements a machine learning pipeline for building a simple chatbot using NLP and a neural network. The chatbot data, containing user input patterns and corresponding response tags, is read from a JSON file. Text preprocessing is applied using tokenization, lowercasing, and stemming to standardize the data. A \"bag-of-words\" model is used to convert each sentence into a numerical vector representation, creating the input features for the model. A simple feedforward neural network is defined with three layers. It takes the bag-of-words vectors as input and predicts the corresponding tag for the input sentence."
   ],
   "id": "6c7e140fe0c90125"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Used imports",
   "id": "8c49a0917fd36c96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:46:06.211975Z",
     "start_time": "2024-12-10T14:46:06.206588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import telegram.ext.filters as filters\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from numpy import ndarray, dtype\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, CallbackContext, MessageHandler\n",
    "from dotenv import load_dotenv"
   ],
   "id": "2a033dd7eb14ad6b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NLP Utils",
   "id": "32f1e0cf718b4111"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Tokenize a string into a list of words using NLTK's word tokenizer.\"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def lower(text: list[str]) -> list[str]:\n",
    "    \"\"\"Convert a list of words to lowercase.\"\"\"\n",
    "    return [w.lower() for w in text]\n",
    "\n",
    "\n",
    "def stem(word: str | list[str]) -> str | list[str]:\n",
    "    \"\"\"Apply stemming to a word or a list of words using Porter Stemmer.\"\"\"\n",
    "    if isinstance(word, list):\n",
    "        return [stemmer.stem(w) for w in word]\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence: list[str], all_words: list[str]) -> np.ndarray[int, np.dtype]:\n",
    "    \"\"\"Create a bag-of-words representation for a tokenized sentence.\"\"\"\n",
    "    tokenized_sentence = stem(tokenized_sentence)\n",
    "    bag = np.zeros(len(all_words), dtype=int)\n",
    "\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "The dataset contains information about tourism in Moldova, categorized into different topics or tags, such as \"tourism_nature\" and \"tourism_history.\" Each entry includes a set of patterns (questions) related to a specific topic, and a set of responses (answers) that provide information about the tourism aspects of Moldova.\n",
    "\n",
    "For example, under the \"tourism_nature\" tag, the patterns are questions related to nature attractions, such as hiking, national parks, and scenic spots in Moldova. The responses offer details about popular nature destinations, such as Orheiul Vechi, Codru Forest, and places suitable for birdwatching or hiking."
   ],
   "id": "cc2794d4db118078"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "    \"tag\": \"tourism_nature\",\n",
    "    \"patterns\": [\n",
    "        \"What natural attractions are there in Moldova?\",\n",
    "        \"Where can I go for nature walks in Moldova?\",\n",
    "        \"What are the most beautiful landscapes in Moldova?\",\n",
    "        \"Are there any national parks in Moldova?\",\n",
    "        \"What are Moldova's top outdoor destinations?\",\n",
    "        \"Can I hike in Moldova?\",\n",
    "        \"What rivers or lakes can I visit in Moldova?\",\n",
    "        \"What are the best spots for birdwatching in Moldova?\"\n",
    "    ],\n",
    "    \"responses\": [\n",
    "        \"Moldova is home to beautiful landscapes, including forests, lakes, and vineyards. Visit places like Orheiul Vechi, a historic complex with stunning views.\",\n",
    "        \"The Codru Forest and the Danube River are popular for nature walks and outdoor activities.\",\n",
    "        \"For hiking and exploration, Tipova Monastery and Saharna Monastery offer picturesque settings.\",\n",
    "        \"Nature reserves like Padurea Domneasca are excellent for birdwatching and wildlife enthusiasts.\"\n",
    "    ]\n",
    "},\n",
    "{\n",
    "    \"tag\": \"tourism_history\",\n",
    "    \"patterns\": [\n",
    "        \"What historical sites can I visit in Moldova?\",\n",
    "        \"Tell me about Moldova's history.\",\n",
    "        \"What are the most famous historical landmarks in Moldova?\",\n",
    "        \"Does Moldova have any castles or ruins?\",\n",
    "        \"Are there any UNESCO World Heritage sites in Moldova?\",\n",
    "        \"What ancient monuments are in Moldova?\",\n",
    "        \"Can I learn about Soviet history in Moldova?\",\n",
    "        \"What are Moldova's key historical periods?\"\n",
    "    ],\n",
    "    \"responses\": [\n",
    "        \"Moldova has a rich history with several key sites like the Orheiul Vechi archaeological complex, the Stefan Cel Mare Park, and the Bender Fortress.\",\n",
    "        \"Moldova is also known for its ancient monasteries, including the CÄƒpriana Monastery.\",\n",
    "        \"Don't miss Soroca Fortress and the National Museum of History in Chisinau for a glimpse into Moldova's past.\",\n",
    "        \"The country offers insights into its Soviet history, with landmarks and museums dedicated to this period.\"\n",
    "    ]\n",
    "},"
   ],
   "id": "2a5ab7e115e3a177"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It is worth noting that having as much patterns as possible is preferable, since the vocabulary and the pattern recognition is directly influenced by the amount of data the model is fed. This is why several questions are given for each tag.",
   "id": "5c8987ad41df40cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparing the dataset",
   "id": "627c5f753a616e36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "all_words: list[str] = []  # List to store all unique words across all patterns\n",
    "tags: list[str] = []  # List to store unique tags (categories) from the dataset\n",
    "xy: list[tuple[list[str], str]] = []  # List of tuples (tokenized_sentence, tag) for each pattern in the dataset\n",
    "bag: ndarray[int, dtype] = []  # Placeholder variable for the bag-of-words representation (used later)\n",
    "IGNORE_WORDS = ['!', '?', '.', ',']  # Words to ignore during preprocessing\n",
    "\n",
    "# Process the dataset\n",
    "for datum in data:\n",
    "    tag = datum['tag']\n",
    "    tags.append(tag)  # Add the tag to the list of tags\n",
    "\n",
    "    for pattern in datum['patterns']:\n",
    "        w = tokenize(pattern)  # Tokenize the pattern into individual words\n",
    "        all_words.extend(w)  # Add all words from the pattern to the list of all_words\n",
    "        xy.append((w, tag))  # Add the tokenized sentence and tag as a tuple to xy\n",
    "\n",
    "# Preprocess all words: remove punctuation, lowercase, and stem for consistency\n",
    "all_words = sorted(set(stem(lower([w for w in all_words if w not in IGNORE_WORDS]))))  # Unique and sorted words\n",
    "tags = sorted(set(lower(tags)))  # Unique and sorted list of tags\n",
    "\n",
    "# Prepare training data\n",
    "X_train = []  # Feature vectors (bag of words for each sentence)\n",
    "Y_train = []  # Labels (index corresponding to the tag)\n",
    "\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # Create a \"bag of words\" representation for each sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)  # Add the feature vector for the sentence\n",
    "    label = tags.index(tag)  # Convert tag to its corresponding numerical index\n",
    "    Y_train.append(label)  # Add the label to the list of labels\n",
    "\n",
    "# Convert training data to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ],
   "id": "530ed486fe1754e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "The code defines a simple feedforward neural network for a classification task, consisting of three layers: an input layer, a hidden layer, and an output layer, with ReLU activations applied after each layer. It is configured with hyperparameters like batch size, hidden layer size, learning rate, and the number of epochs for training. The model is designed to classify inputs into multiple categories based on the number of tags (output classes) and is initialized to run on either a CPU or GPU, depending on availability."
   ],
   "id": "38942e0d4e003318"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Define a simple feedforward neural network with 3 layers\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)  # Input to hidden layer\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)  # Hidden to hidden\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)  # Hidden to output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass using a list for clarity\n",
    "        apply_order = [self.l1, self.relu, self.l2, self.relu, self.l3]\n",
    "\n",
    "        for f in apply_order:  # Sequentially apply layers and activations\n",
    "            x = f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Hyperparameters and model configuration\n",
    "BATCH_SIZE = 8\n",
    "HIDDEN_SIZE = 128\n",
    "OUTPUT_SIZE = len(tags)  # Number of classes (tags)\n",
    "INPUT_SIZE = len(X_train[0])  # Feature vector size\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 200\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Check if GPU is available\n",
    "print('Running on device:', device)\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "model = NN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE).to(device)"
   ],
   "id": "6dd70e73f492f7be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "The train function defines and runs the training process for the neural network model. It first creates a custom dataset class that loads the training data and implements methods to access individual samples and the dataset size. The training loop uses DataLoader to batch the data and shuffle it for better generalization. The loss function used is CrossEntropyLoss, which is suitable for multi-class classification. The optimizer is Adam, which updates the model's weights during training. In each epoch, the model's predictions are compared with the true labels, and the loss is backpropagated to update the model parameters. Every 5 epochs, the loss is printed. Once training is complete, the model's state is saved to a file."
   ],
   "id": "2c787bdd8a495eef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train():\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self):\n",
    "            self.n_samples = len(X_train)\n",
    "            self.x_data = X_train  # Features (bag of words)\n",
    "            self.y_data = Y_train  # Labels (numerical indices for tags)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.x_data[index], self.y_data[index]\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.n_samples\n",
    "\n",
    "    print('Training...')\n",
    "\n",
    "    dataset = CustomDataset()\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    loss: nn.CrossEntropyLoss() | None = None  # Placeholder to store the loss during training\n",
    "\n",
    "    # Training loop over the specified number of epochs\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch, (words, labels) in enumerate(train_loader):\n",
    "            # Convert features and labels to the correct type and move to the computation device\n",
    "            words = words.float().to(device)  # Convert features to float tensor\n",
    "            labels = labels.to(device)  # Move labels to the same device\n",
    "\n",
    "            # Forward pass: compute predictions from the model\n",
    "            output = model(words)\n",
    "\n",
    "            # Compute the loss between predictions and true labels\n",
    "            loss = loss_fn(output, labels)\n",
    "\n",
    "            # Backpropagation: clear previous gradients, compute new ones, and update parameters\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "        if epoch % 5 == 4:\n",
    "            print(f'Epoch [{epoch + 1}/{EPOCHS}] Loss: {loss.item():.10f}')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    print(f'Finished training with final loss {loss.item():.10f}')\n"
   ],
   "id": "f095e65a464f9431"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bot\n",
    "\n",
    "This code defines a TelegramBot class that interacts with the Telegram API using the python-telegram-bot library. It initializes the bot with a Telegram API key from environment variables and sets up the bot to listen for messages. The start() method begins polling for incoming messages. When a message is received, the _handle_message() method is called, which attempts to generate a response using the generate_response() function. If successful, it sends the response back to the user; if an error occurs, it sends an error message instead."
   ],
   "id": "325d880707c86994"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TelegramBot:\n",
    "    def __init__(self):\n",
    "        key = os.environ.get('TELEGRAM_KEY')\n",
    "        self.app = ApplicationBuilder().token(key).build()\n",
    "\n",
    "    def start(self):\n",
    "        self.app.add_handler(MessageHandler(filters.CHAT, self._handle_message))\n",
    "        self.app.run_polling()\n",
    "\n",
    "    async def _handle_message(self, update: Update, context: CallbackContext) -> None:\n",
    "        msg = update.message.text\n",
    "        chat_id = update.effective_chat.id\n",
    "\n",
    "        # Failure handling\n",
    "        try:\n",
    "            res = generate_response(msg)\n",
    "            await context.bot.send_message(chat_id=chat_id, text=res)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            await context.bot.send_message(chat_id=chat_id, text=f'Error generating response: {e}')\n"
   ],
   "id": "993fda978e98b332"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusions\n",
    "\n",
    "Using AI and ML is a good approach in developing a chatbot, especially if the bot needs to have a more human-like natural language understanding. These technologies enable the chatbot to process, interpret, and respond to user inputs more accurately, improving user experience and making interactions feel more natural and intuitive."
   ],
   "id": "6ed31c78af0b4846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The use of a simple feedforward neural network to classify patterns in user queries is a good starting point, but more sophisticated models, such as sequence-to-sequence , offer even greater potential for handling more complex interactions. Seq2Seq models, in particular, are a powerful architecture for tasks involving sequence generation, such as machine translation, text summarization, and chatbot dialogue systems. By using encoder-decoder structures, Seq2Seq models excel at understanding and generating variable-length responses, making them ideal for conversational agents. These models can handle contextual information across multiple turns in a conversation, which is a crucial feature for building intelligent chatbots. Using Seq2Seq with attention mechanisms further enhances its ability to focus on relevant parts of the input sequence while generating an appropriate response. This capability allows for more dynamic and contextually accurate interactions. As AI systems grow more sophisticated, combining robust preprocessing techniques with advanced models like Seq2Seq paves the way for developing powerful, user-friendly conversational systems capable of providing meaningful and accurate responses across a wide range of domains.",
   "id": "dc58826ee3f15bc6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
